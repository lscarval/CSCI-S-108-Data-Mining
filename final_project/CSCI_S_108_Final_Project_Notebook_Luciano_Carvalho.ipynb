{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0e64e2",
   "metadata": {},
   "source": [
    "\n",
    "# CSCI S-108: Data Mining, Discovery, and Exploration\n",
    "## Final Project: Optimizing Data Egress for Secure Transmission Using Data Mining Techniques\n",
    "**Student**: Luciano Carvalho\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "#### Objective\n",
    "The objective of this project is to develop a secure and efficient method for data egress, utilizing advanced data mining techniques to obfuscate sensitive data and reduce the size of datasets before transmission.\n",
    "\n",
    "#### Background\n",
    "As the Director of DevOps at Intelex Technologies, my role includes managing the company's databases and ensuring secure data handling. This project focuses on addressing the challenges of handling large data exports, obfuscating sensitive information, and optimizing data transmission.\n",
    "\n",
    "#### Dataset\n",
    "We use the UNSW-NB15 dataset, which contains network traffic data with multiple features such as IP addresses, port numbers, and protocols. The dataset includes labels indicating normal and various attack types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0690e31",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Problem Statement\n",
    "\n",
    "The key challenges addressed in this project include the lack of robust solutions for secure data obfuscation and size reduction during data transmission. Additionally, the project aims to tackle operational challenges in monitoring and managing deployments across multiple platforms (AWS, Azure, Rackspace).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ab983",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "#### 3.1 Data Loading and Description\n",
    "This section covers the initial data loading, examination of data structure, and basic statistical description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a736088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('UNSW-NB15_1.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first few rows and summary statistics\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dd051",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.2 Data Cleaning\n",
    "We handle missing values, convert data types, and perform basic preprocessing to ensure data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handling missing values and data type conversion\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eba2bb",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.3 Data Visualization\n",
    "We use various visualization techniques to explore the data, including histograms, box plots, and correlation matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histograms and box plots for key features\n",
    "sns.histplot(df['dur'], kde=True, log_scale=(True, False))\n",
    "plt.title('Distribution of Duration (dur) with Logarithmic Scale')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boxplot for outlier detection\n",
    "sns.boxplot(data=df[['Spkts', 'Dpkts']])\n",
    "plt.title('Boxplot of Spkts and Dpkts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaed7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation Matrix\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928f5c4",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Methodology\n",
    "\n",
    "In this section, we explore various data mining methods applicable to our project, including data exploration, streaming analysis, clustering, dimensionality reduction, similarity search, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469127ca",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.1 Data Exploration and Analysis Techniques\n",
    "We begin with a thorough exploration of the dataset, using techniques like summary statistics, data visualization, and more, as discussed in the \"Pitfalls in Data Mining\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca38ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary Statistics\n",
    "df.describe()\n",
    "\n",
    "# Visualization of distribution for numerical features\n",
    "for column in numeric_columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5650f",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.2 Streaming Analysis\n",
    "We implement streaming analytics methods such as moving averages and exponential smoothing, using concepts from the \"Streaming\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Moving average calculation\n",
    "df['moving_avg'] = df['dur'].rolling(window=5).mean()\n",
    "\n",
    "# Plot the moving average\n",
    "plt.plot(df['moving_avg'])\n",
    "plt.title('Moving Average of Duration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be76864",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.3 Similarity Search\n",
    "We employ techniques like KD-Trees and Locality Sensitive Hashing (LSH) to find similar data points or patterns, based on the \"Efficient Similarity Search\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da302d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of KD-Trees\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# Using a subset of data for demonstration\n",
    "subset = df.sample(n=1000)\n",
    "kdt = KDTree(subset[numeric_columns], leaf_size=30, metric='euclidean')\n",
    "\n",
    "# Find nearest neighbors for a sample point\n",
    "dist, ind = kdt.query(subset[numeric_columns].iloc[:1], k=5)\n",
    "print(f'Indices of 5 nearest neighbors: {ind}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75dc61",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.4 Recommender Systems\n",
    "Discussion on the implementation of simple collaborative filtering or content-based filtering methods, inspired by the \"Recommenders\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder: Example of a simple recommender system approach\n",
    "# Example: Recommend top 5 similar activities based on historical data (placeholder logic)\n",
    "# (This could be expanded based on the available data and specific use case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3767079",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.5 Clustering Models\n",
    "We apply clustering algorithms such as DBSCAN, K-Means, and Hierarchical Clustering, and evaluate their performance, following the \"Cluster Models\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of DBSCAN clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.5, min_samples=5).fit(df[numeric_columns])\n",
    "df['cluster'] = db.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "plt.scatter(df['pca1'], df['pca2'], c=df['cluster'])\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('DBSCAN Clustering Result')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f563f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of K-Means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['kmeans_cluster'] = kmeans.fit_predict(df[numeric_columns])\n",
    "\n",
    "# Visualize K-Means clusters\n",
    "plt.scatter(df['pca1'], df['pca2'], c=df['kmeans_cluster'])\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('K-Means Clustering Result')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c764fd",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.6 Dimensionality Reduction\n",
    "We use PCA and t-SNE to reduce data dimensions and visualize the reduced data, as explored in the \"Dimensionality Reduction\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df[numeric_columns])\n",
    "df['pca1'], df['pca2'] = pca_result[:, 0], pca_result[:, 1]\n",
    "\n",
    "# Plot PCA results\n",
    "plt.scatter(df['pca1'], df['pca2'], c=df['cluster'])\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA Result')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f5560",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.7 Association Models\n",
    "Applying association rule mining to discover relationships between variables, inspired by the \"Association Models\" assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of association rule mining\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Placeholder: Sample transactions data (this should be replaced with actual data)\n",
    "transactions = pd.DataFrame({\n",
    "    'items': [['A', 'B', 'C'], ['A', 'C'], ['B', 'C'], ['A', 'B'], ['A', 'B', 'C']]\n",
    "})\n",
    "\n",
    "# Generate frequent itemsets\n",
    "frequent_itemsets = apriori(transactions, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "rules.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a9d27",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Results and Discussion\n",
    "\n",
    "#### 5.1 Evaluation of Clustering Models\n",
    "We evaluate the clustering models using metrics such as Silhouette Score, Davies-Bouldin Index, and visualizations to interpret the effectiveness of the clustering.\n",
    "\n",
    "#### 5.2 Dimensionality Reduction Insights\n",
    "We analyze the results from PCA and t-SNE to understand how the dimensionality reduction techniques preserve data variance and structure.\n",
    "\n",
    "#### 5.3 Streaming Data Analysis Observations\n",
    "Observations from the streaming data analysis, including the impact of applying moving averages and filters on the dataset.\n",
    "\n",
    "#### 5.4 Similarity Search Results\n",
    "We discuss the findings from KD-Trees and LSH, focusing on their effectiveness in identifying similar data points or patterns.\n",
    "\n",
    "#### 5.5 Recommender Systems Analysis\n",
    "Although not the primary focus, we explore the feasibility of implementing a recommender system based on the dataset's characteristics, including potential use cases and challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97953a46",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Conclusion\n",
    "\n",
    "This project successfully demonstrated the application of various data mining techniques to enhance data egress security and efficiency. By leveraging clustering, dimensionality reduction, streaming data analysis, and similarity search, we addressed the primary challenges faced by Intelex Technologies in handling large datasets securely and effectively.\n",
    "\n",
    "#### Key Takeaways\n",
    "- Clustering models provided valuable insights into data structure and patterns.\n",
    "- Dimensionality reduction techniques helped in visualizing high-dimensional data.\n",
    "- Streaming data analysis facilitated real-time data processing and monitoring.\n",
    "- Similarity search proved useful for identifying similar data points, enhancing data obfuscation and security.\n",
    "\n",
    "#### Future Work\n",
    "Further exploration could include the integration of more advanced machine learning models, the development of a full-fledged recommender system, and the continuous monitoring of data streams for real-time anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1cdea",
   "metadata": {},
   "source": [
    "\n",
    "### 7. References\n",
    "\n",
    "- **Scikit-Learn**: Pedregosa et al., JMLR 2011.\n",
    "- **Seaborn**: Michael Waskom (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021.\n",
    "- **Pandas**: Wes McKinney (2010). Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference.\n",
    "- **UNSW-NB15 Dataset**: A comprehensive dataset for network intrusion detection systems (NIDS).\n",
    "- **Apriori and Association Rules**: Agrawal et al., Mining association rules between sets of items in large databases, SIGMOD '93.\n",
    "- Additional references to be included based on the tools and techniques applied.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
